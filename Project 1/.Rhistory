library(stringr)
library(tidyquant)
library(tidyquant)
install.packages('RTextTools')
install.packages('tm')
install.packages('SnowballC')
install.packages('broom')
install.packages('pdftools')
install.packages("broom")
library(RTextTools)
library(RTextTools)
library(tm)
library(tm)
library(SnowballC)
library(SnowballC)
library(broom)
library(pdftools)
library(tau)
library(wordcloud)
library(wordcloud)
# 1.Creating a matrix
data(USCongress)
View(USCongress)
View(USCongress)
create_matrix?
doc_matrix <- create_matrix(USCongress$text, language="english", removeNumbers=TRUE,
stemWords=TRUE, removeSparseTerms=.998)
create_matrix?
doc_matrix <- create_matrix(USCongress$text, language="english", removeNumbers=TRUE,
stemWords=TRUE, removeSparseTerms=.998)
?create_matrix
# 2.Creating a container
?create_container
# 2.Creating a container
#?create_container
container <- create_container(doc_matrix, USCongress$major, trainSize=1:4000,
testSize=4001:4449, virgin=FALSE)
View(doc_matrix)
View(doc_matrix)
View(container)
View(container)
# 3.Training models
?train_model
print_algorithms()
SVM <- train_model(container,"SVM")
View(SVM)
View(SVM)
# 4.Classifying data using trained models
?classify_model()
# 5.Analytics
?create_analytics
# 5.Analytics
#?create_analytics
analytics <- create_analytics(container,
cbind(SVM_CLASSIFY))
# 4.Classifying data using trained models
#?classify_model()
SVM_CLASSIFY <- classify_model(container, SVM)
View(SVM_CLASSIFY)
View(SVM_CLASSIFY)
# 5.Analytics
#?create_analytics
analytics <- create_analytics(container,
cbind(SVM_CLASSIFY))
View(analytics)
View(analytics)
summary(analytics)
topic_summary <- analytics@label_summary
View(topic_summary)
View(topic_summary)
alg_summary <- analytics@algorithm_summary
ens_summary <-analytics@ensemble_summary
doc_summary <- analytics@document_summary
# 6.Cross-Validation
SVM <- cross_validate(container, 4, "SVM")
# Load packages
library(stringr)
library(tidyquant)
library(RTextTools)
library(tm)
library(SnowballC)
library(broom)
library(pdftools)
library(tau)
library(wordcloud)
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc","RCurl","XML")
install.packages(Needed, dependencies=TRUE)
install.packages(Needed, dependencies = TRUE)
install.packages(Needed, dependencies = TRUE)
install.packages(Needed, dependencies = TRUE)
install.packages(Needed, dependencies = TRUE)
install.packages(Needed, dependencies = TRUE)
install.packages(Needed, dependencies = TRUE)
# Load packages
library(stringr)
library(tidyquant)
library(RTextTools)
library(tm)
library(SnowballC)
library(broom)
library(pdftools)
library(tau)
library(wordcloud)
data("NYTimes")
nyt.data <- NYTimes
mycorpus <- VCorpus(VectorSource(nyt.data$Title))
data("NYTimes")
nyt.data <- NYTimes
mycorpus <- VCorpus(VectorSource(nyt.data$Title))
View(nyt.data)
View(nyt.data)
View(mycorpus)
View(mycorpus)
inspect(mycorpus[[2]])
clean_corpus <- function(corpus){
mycorpus<-tm_map(mycorpus,content_transformer(tolower))
mycorpus<-tm_map(mycorpus,removePunctuation)
mycorpus<-tm_map(mycorpus,removeNumbers)
mycorpus<-tm_map(mycorpus,removeWords,stopwords("english"))
mycorpus<-tm_map(mycorpus,stripWhitespace)
#mycorpus<-tm_map(mycorpus,PlainTextDocument)
#dictCorpus<-mycorpus
mycorpus<-tm_map(mycorpus,stemDocument)
#mycorpus<-tm_map(mycorpus,stemCompletion,dictCorpus)
return(mycorpus)
}
mycorpus = clean_corpus(mycorpus)
inspect(mycorpus[[2]])
#build Term Document Matrix
?TermDocumentMatrix
myTDM<-TermDocumentMatrix(mycorpus, control=list(minWordLength=1))
myTDMdf<-as.data.frame(as.matrix(t(myTDM)))
View(myTDMdf)
View(myTDMdf)
head(myTDMdf)
#find most frequent terms
findFreqTerms(myTDM,lowfreq=100)
#find associations
findAssocs(myTDM, terms='war', 0.1)
#find associations
?findAssocs
findAssocs(myTDM, terms='war', 0.2)
#n-grams analysis
mydf<-data.frame(text=unlist(sapply(mycorpus,'[',"content")),stringAsFactors=FALSE)
View(mydf)
View(mydf)
mydf$text<-as.character(mydf$text)
View(mydf)
View(mydf)
?textcnt
ngram_1L<-textcnt(mydf$text,n=1L,method="string")
max(ngram_1L)
ngram_t1<-data.frame(counts=unclass(ngram_1L),size=nchar(names(ngram_1L)),text=names(ngram_1L))
View(ngram_t1)
View(ngram_t1)
max(ngram_t1$counts)
n1L<-arrange(ngram_t1,desc(counts))
View(n1L)
counts1<-n1L[,c(3,1)]
View(counts1)
View(counts1)
ngram_3L<-textcnt(mydf$text,n=3L,method="string")
ngram_t3<-data.frame(counts=unclass(ngram_3L),size=nchar(names(ngram_3L)),text=names(ngram_3L))
n3L<-arrange(ngram_t3,desc(counts))
counts3<-n3L[,c(3,1)]
View(counts3)
#wordcloud
wordcloud(counts1$text,counts1$count,min.freq=50)
#finding the types of articles
#k-means clustering
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
for(i in 1:15){wss[i]<-sum(kmeans(myTDMdf,centers=i)$withinss)}
#finding the types of articles
#k-means clustering
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
#finding the types of articles
#k-means clustering
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
plot(1:15,wss,type="b",xlab="No. of Clusters",ylab="wss")
k<-kmeans(myTDMdf,5,nstart=20)
groups<-data.frame(k$cluster)
table(groups)
#finding the types of articles
#k-means clustering
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
#finding the types of articles
#k-means clustering
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
for(i in 1:15){wss[i]<-sum(kmeans(myTDMdf,centers=i)$withinss)}
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
for(i in 1:15){wss[i]<-sum(kmeans(myTDMdf,centers=i)$withinss)}
plot(1:15,wss,type="b",xlab="No. of Clusters",ylab="wss")
wss<-(nrow(myTDMdf)-1)*sum(apply(myTDMdf,2,var))
for(i in 1:15){wss[i]<-sum(kmeans(myTDMdf,centers=i)$withinss)}
plot(1:15,wss,type="b",xlab="No. of Clusters",ylab="wss")
?kmeans
plot(1:15,wss,type="b",xlab="No. of Clusters",ylab="wss")
k<-kmeans(myTDMdf,5,nstart=20)
groups<-data.frame(k$cluster)
View(k)
table(groups)
title<-as.data.frame(nyt.data [,"Title"])
finalDF<-as.data.frame(cbind(title,groups))
View(finalDF)
View(finalDF)
names(finalDF)<-c("title","group")
x1<-subset(finalDF, group==2)
View(x1)
N      <- nrow(nyt.data)
Ntrain <- 1500
Ntest  <- N - Ntrain
sparseThreshold <- 0.01
nyt.matrix<-create_matrix(nyt.data$Title,
language="english",
removeNumbers = TRUE,
stemWords = TRUE,
removeSparseTerms=(1-sparseThreshold),
weighting = weightTfIdf)
nyt.container <-create_container(nyt.matrix,
nyt.data$Topic.Code,
trainSize = 1:Ntrain,
testSize = (Ntrain+1):N,
virgin=FALSE)
nyt.SVM <- train_model(nyt.container,"SVM")
nyt.SVM_CLASSIFY <- classify_model(nyt.container, nyt.SVM)
nyt.analytics <- create_analytics(nyt.container,nyt.SVM_CLASSIFY)
summary(nyt.analytics)
